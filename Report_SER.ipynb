{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "You are working as an ML engineer in Visiemo, a famous sound-based sentiment analysis company. It’s Monday morning and you are chilling at the coffee machine with fellow Visiumees. Suddenly, the AI project manager schedules you a project kick-off meeting. A multinational client is interested in our services: the client has different call centers around the world and would like to analyze the satisfaction of its customers based on the recordings of the calls.\n",
    "\n",
    "The client has heard about AI and Visiemo, more precisely about our expertise in sentiment analysis.\n",
    "However,the client is reluctant in whether the solution will work. As such, it has been agreed to first do a proof-of-concept on the call centers from Germany.\n",
    "\n",
    "At Visiemo, we have decided to use an already existing dataset and design a small proof-of-concept to showcase the solution to the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In the frame of this use-case, you will be confronted to **Speech Emotion Recognition** with the **Emo-DB** dataset. The goal is to design a proof-of-concept model which classifies samples of emotional speech in one of the given emotions.\n",
    "\n",
    "By the end of the use-case, you should have a working model with good enough performance, fairly evaluated. \n",
    "\n",
    "*We do not aim to have the best performing model.* On the contrary, you should aim for an end-to-end working solution that enables inference on new speech samples.\n",
    "\n",
    "* As a **report**, you should fill in the present notebook.\n",
    "* You should dockerize your project. The **Docker container** should handle everything (download the data, run the code, etc.). When running the container **it should expose the Jupyter notebook on one port and expose a Flask API on another one**.\n",
    "* The **Flask API** should contain two endpoints:\n",
    "  - One for training the model\n",
    "  - One for querying the last trained model with an audio file of our choice in the dataset\n",
    "* The code that you will develop should be **entirely reproducible** and **documented in a README.md file**.\n",
    "\n",
    "## Final presentation\n",
    "\n",
    "* The final presentation will be held on-site or remotely depending on your availability. \n",
    "* An AI Project Manager and two ML engineers will be present. \n",
    "* You will start by giving a 10-15 min technical presentation about your approach and your findings, using the slides provided as deliverables. Finally, a discussion with questions of about 20 min will follow.\n",
    "* It should be clear enough to understand your approach and your decisions without going too much in the code. \n",
    "* You can also include code snippets, demos, etc.\n",
    "\n",
    "## Important points to keep in mind\n",
    "\n",
    "* Please be aware that this use-case might take a substantial amount of time, especially if you are not familiar with all the tools.\n",
    "* You will have one full week to tackle the problem, starting today. We broadly estimate to *15-20 hours* the usual development time needed.\n",
    "* Don’t spend too much time finding the perfect technical solution or making tiny metric improvements. \n",
    "* If you don't manage to finish all the tasks, you can still send us your use-case: we will discuss the difficulties you encountered during the final presentation.\n",
    "* **Do not upload your code to Github, or at least not in a public repo.**\n",
    "* Feel free to use any available library you would need, but **do not re-use someone else's code without mentioning it.**\n",
    "\n",
    "## Deliverables \n",
    "\n",
    "Please provide the following elements in a **.zip** file as deliverables:\n",
    "* This report filled with your approach, in the form of an **iPython Notebook**.\n",
    "* A **5-10 slides PDF file**, containing a technical presentation covering the important aspects of your work.\n",
    "* A **Dockerfile** defining a container for the project expositing the present notebook and the Flask API.\n",
    "* A **README.md** which should contain the commands to build and run the docker container, as well as how to perform the queries to the API. \n",
    "* **Any** necessary .py, .sh or other files needed to run your code.\n",
    "\n",
    "You will be judged on your technical approach, evaluation, visualizations, coding and presentation skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found [here](http://emodb.bilderbar.info/index-1280.html).\n",
    "\n",
    "It contains samples of emotional speech in German, labeled with one of 7 different emotions: Anger, Boredom, Disgust, Fear, Happiness, Sadness and Neutral. \n",
    "\n",
    "Please download the full database and refer to the documentation to understand how the samples are labeled (see \"Additional information\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import sklearn\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from skimage import io\n",
    "from torchvision.utils import make_grid\n",
    "from utils import pytorchtools\n",
    "from utils import preprocessing\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "from torchmetrics import PrecisionRecallCurve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\tanger\t                    W\tÄrger (Wut)\n",
    "# B\tboredom\t                    L\tLangeweile\n",
    "# D\tdisgust\t                    E\tEkel\n",
    "# F\tanxiety/fear\t            A\tAngst\n",
    "# H\thappiness\t                F\tFreude\n",
    "# S\tsadness\t                    T\tTrauer\n",
    "    \n",
    "def decompose_emodb(EMODB_PATH):\n",
    "    'Name Folder same as file extension'\n",
    "    # EMODB_PATH = './dataset/wav/'\n",
    "    emotion = []\n",
    "    path = []\n",
    "    for root, dirs, files in os.walk(EMODB_PATH):\n",
    "        for name in files:\n",
    "            if name[5] == 'W':  # Ärger (Wut) -> Angry\n",
    "                emotion.append('angry')\n",
    "            elif name[5] == 'L':  # Langeweile -> Boredom\n",
    "                emotion.append('bored')\n",
    "            elif name[5] == 'E':  # Ekel -> Disgusted\n",
    "                emotion.append('disgust')\n",
    "            elif name[5] == 'A':  # Angst -> Angry\n",
    "                emotion.append('fear')\n",
    "            elif name[5] == 'F':  # Freude -> Happiness\n",
    "                emotion.append('happy')\n",
    "            elif name[5] == 'T':  # Trauer -> Sadness\n",
    "                emotion.append('sad')\n",
    "            elif name[5] == 'N':\n",
    "                emotion.append('neutral')\n",
    "            else:\n",
    "                emotion.append('unknown')\n",
    "            path.append(os.path.join(EMODB_PATH, name))\n",
    "\n",
    "    emodb_df = pd.DataFrame(emotion, columns=['labels'])\n",
    "    emodb_df = pd.concat([emodb_df, pd.DataFrame(path, columns=['path'])], axis=1)\n",
    "    \n",
    "    return emodb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='labels', ylabel='count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_summary = decompose_emodb('./dataset/wav/')\n",
    "sns.countplot(x ='labels',data=dataset_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>angry</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bored</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disgust</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sad</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    labels  count\n",
       "0    angry    127\n",
       "1    bored     81\n",
       "2  disgust     46\n",
       "3     fear     69\n",
       "4    happy     71\n",
       "5  neutral     79\n",
       "6      sad     62"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dist = dataset_summary.groupby('labels').count().reset_index()[['labels','path']]\n",
    "class_dist.columns = ['labels','count']\n",
    "class_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "sample_rate, samples = wavfile.read('./dataset/wav/09b03Ed.wav')\n",
    "frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import plot_mel\n",
    "\n",
    "for path in dataset_summary['path']:\n",
    "    jpeg_path = path.replace('wav','jpeg')\n",
    "    audio, rate = librosa.load(path)\n",
    "    fig = plot_mel(audio,rate)\n",
    "    fig.savefig(jpeg_path)\n",
    "    # convert it to a numpy array\n",
    "    numpy_image = mplfig_to_npimage(fig)\n",
    "    torch.tensor(numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_summary = decompose_emodb('./dataset/jpeg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fear', 'disgust', 'angry', 'bored', 'neutral', 'happy', 'sad'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_summary.labels.unique()\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(labels)\n",
    "le.inverse_transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.label_encoder_to_json(le,'labels.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_summary.replace(labels,le.fit_transform(labels),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>./dataset/jpeg/12b02Ad.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>./dataset/jpeg/14a02Ea.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/12b09Wc.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/10a05Wb.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/08a01Wc.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/15b03Wa.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/13a02Wa.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/09b03Wb.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>5</td>\n",
       "      <td>./dataset/jpeg/03b09Nc.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>1</td>\n",
       "      <td>./dataset/jpeg/09b03Lb.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels                         path\n",
       "0         3  ./dataset/jpeg/12b02Ad.jpeg\n",
       "1         2  ./dataset/jpeg/14a02Ea.jpeg\n",
       "2         0  ./dataset/jpeg/12b09Wc.jpeg\n",
       "3         0  ./dataset/jpeg/10a05Wb.jpeg\n",
       "4         0  ./dataset/jpeg/08a01Wc.jpeg\n",
       "..      ...                          ...\n",
       "530       0  ./dataset/jpeg/15b03Wa.jpeg\n",
       "531       0  ./dataset/jpeg/13a02Wa.jpeg\n",
       "532       0  ./dataset/jpeg/09b03Wb.jpeg\n",
       "533       5  ./dataset/jpeg/03b09Nc.jpeg\n",
       "534       1  ./dataset/jpeg/09b03Lb.jpeg\n",
       "\n",
       "[535 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 0, 1, 5, 4, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_label = self.df.iloc[idx]['labels']\n",
    "        image_path = self.df.iloc[idx]['path']\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        return (image, image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>./dataset/jpeg/12b02Ad.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>./dataset/jpeg/14a02Ea.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/12b09Wc.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/10a05Wb.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/08a01Wc.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/15b03Wa.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/13a02Wa.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/jpeg/09b03Wb.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>5</td>\n",
       "      <td>./dataset/jpeg/03b09Nc.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>1</td>\n",
       "      <td>./dataset/jpeg/09b03Lb.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels                         path\n",
       "0         3  ./dataset/jpeg/12b02Ad.jpeg\n",
       "1         2  ./dataset/jpeg/14a02Ea.jpeg\n",
       "2         0  ./dataset/jpeg/12b09Wc.jpeg\n",
       "3         0  ./dataset/jpeg/10a05Wb.jpeg\n",
       "4         0  ./dataset/jpeg/08a01Wc.jpeg\n",
       "..      ...                          ...\n",
       "530       0  ./dataset/jpeg/15b03Wa.jpeg\n",
       "531       0  ./dataset/jpeg/13a02Wa.jpeg\n",
       "532       0  ./dataset/jpeg/09b03Wb.jpeg\n",
       "533       5  ./dataset/jpeg/03b09Nc.jpeg\n",
       "534       1  ./dataset/jpeg/09b03Lb.jpeg\n",
       "\n",
       "[535 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0.8 * 0.125 gives 0.1 for validation\n",
    "train_df,test_df = train_test_split(dataset_summary, test_size=0.2,stratify=dataset_summary.labels)\n",
    "train_df,valid_df = train_test_split(train_df,test_size=0.125,stratify=train_df.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='labels', ylabel='count'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x ='labels',data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='labels', ylabel='count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x ='labels',data=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='labels', ylabel='count'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x ='labels',data=valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True,drop=True)\n",
    "test_df.reset_index(inplace=True,drop=True)\n",
    "valid_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.Resize((256,256)),\n",
    "                             T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmoDataset(train_df, transform=resize)\n",
    "test_dataset = EmoDataset(test_df, transform=resize)\n",
    "valid_dataset = EmoDataset(valid_df, transform=resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:32], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "import torch as torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,l in valid_loader:\n",
    "    print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)\n",
    "\n",
    "\n",
    "##REPLACE LAST LAYER\n",
    "num_labels = len(dataset_summary['labels'].unique())\n",
    "model.classifier[6] = torch.nn.Linear(4096,num_labels)\n",
    "#Freeze the gradients of all of the layers in the features (convolutional) layers\n",
    "# for param in model.features.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "params = {'learning_rate':1e-5,'epochs':200, 'patience':20}\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "running_loss = 0\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to track the training loss as the model trains\n",
    "train_losses = []\n",
    "# to track the validation loss as the model trains\n",
    "valid_losses = []\n",
    "# to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "# to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy(model, test_loader, device):\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images,labels in test_loader:\n",
    "            # run the model on the test set to predict labels\n",
    "            images=images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).to(device)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (accuracy / total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(train_loader.dataset)\n",
    "print(size)\n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = pytorchtools.EarlyStopping(patience=params['patience'],verbose=True)\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    #######TRAIN MODEL########\n",
    "    epochs_loss=0\n",
    "\n",
    "    model.train()\n",
    "    for i, (images,labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprpagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #calculate train_loss\n",
    "        train_losses.append(loss.item())\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "        optimizer.step()\n",
    "    ##########################    \n",
    "    #####TEST MODEL#######\n",
    "    ##########################    \n",
    "    accuracy = testAccuracy(model, test_loader, device)\n",
    "    ##########################    \n",
    "    #####VALIDATE MODEL#######\n",
    "    ##########################\n",
    "    model.eval()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs,labels)\n",
    "        valid_losses.append(loss.item())\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    #print(train_loss)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "    print_msg = (f'accuracy: {accuracy:.3f} ' + f'train_loss: {train_loss:.3f} ' + f'valid_loss: {valid_loss:.3f}')\n",
    "    \n",
    "    print(print_msg)\n",
    "\n",
    "    # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    early_stopping(valid_loss, model)\n",
    "    print(epoch)\n",
    "        \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(test_loader, model, label_encoder, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        data, target = images.to(device), labels.to(device)\n",
    "        output = model(data)  # shape = torch.Size([batch_size, 10])\n",
    "        pred = output.argmax(\n",
    "            dim=1, keepdim=True\n",
    "        )  # pred will be a 2d tensor of shape [batch_size,1]\n",
    "\n",
    "        preds.append(pred.flatten().to(\"cpu\").numpy())\n",
    "        true_labels.append(labels.flatten().to(\"cpu\").numpy())\n",
    "\n",
    "    #### GET MISIDENTIFIED EXAMPLE\n",
    "    all_preds = np.concatenate(preds, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    df = pd.DataFrame({\"expected\": true_labels, \"predicted\": all_preds})\n",
    "\n",
    "    ### DECODE LABELS in datafram\n",
    "    for col in df:\n",
    "        df[col] = label_encoder.inverse_transform(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "errs = error_analysis(test_loader,model, le, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "######1st plot#########\n",
    "ax1 = fig.add_subplot()\n",
    "ax1.set_ylabel('valid  /  train loss')\n",
    "ax1.set_xlabel('number of epochs')\n",
    "halt = avg_valid_losses.index(min(avg_valid_losses))\n",
    "\n",
    "\n",
    "plt.axvline(x=halt, color='r', linestyle=\"--\", label=\"stop training\")\n",
    "\n",
    "print(avg_valid_losses.index(min(avg_valid_losses)))\n",
    "\n",
    "\n",
    "plt.plot(list(range(len(avg_valid_losses))), avg_valid_losses, label = \"valid loss\")\n",
    "plt.plot(list(range(len(avg_valid_losses))), avg_train_losses, label=\"train loss\")\n",
    "\n",
    "\n",
    "h,labels = ax1.get_legend_handles_labels()\n",
    "labels[:1] = ['stop training','valid loss', 'train loss',]\n",
    "ax1.legend(labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_confusion_matrix(y_true,y,classes):\n",
    "    cf_matrix = confusion_matrix(y_true, y)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cf_matrix, annot=True)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(classes)\n",
    "    ax.yaxis.set_ticklabels(classes)\n",
    "    plt.savefig('./matrix.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(errs['expected'], errs['predicted'],le.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
